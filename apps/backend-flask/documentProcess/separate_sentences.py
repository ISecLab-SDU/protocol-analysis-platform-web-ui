import re
import json
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_random_exponential
from openai import OpenAI
import concurrent.futures

from tqdm import tqdm
import toml
from pathlib import Path

script_dir = Path(__file__).parent.parent
config_path = script_dir / "config.toml"
prompt_path = script_dir / "prompt.toml"
toml_config = toml.load(config_path)
toml_prompt = toml.load(prompt_path)
this_workers = toml_config["llm"]["max_workers"]
this_url = toml_config["llm"]["base_url"]
PROMPT_TEMPLATE = toml_prompt["prompt_separate_sentences"]["user"]
this_model = toml_config["llm"]["model1"]

def sanitize_text(text: str, limit: int = 10000) -> str:
    if text and len(text) > limit:
        return text[:limit] + "\n...[TRUNCATED]..."
    return text or ""

@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=10))
def process_sentence(client, heading, sentence):
    prompt = PROMPT_TEMPLATE.format(heading=sanitize_text(heading, 2000), sentence=sanitize_text(sentence, 12000)) 
    response = client.chat.completions.create(
        model=this_model,
        messages=[
            {"role": "user", "content": prompt}
        ],
        response_format={
            'type': 'json_object'
        }
    )
    return response.choices[0].message.content.strip()


def process_sentences(config):
    print("ğŸ”„ Processing sentences...")

    # åŠ è½½æ•°æ®
    with open(config["paths"]["text_processed"], "r") as f:
        headings = json.load(f)
    with open(config["paths"]["headings"], "r") as f:
        selected = json.load(f)

    client = OpenAI(api_key=config["api_key"], base_url=this_url)
    results = {}
    df_data = []

    # ä½¿ç”¨tqdmè¿›åº¦æ¡
    with tqdm(total=len(selected), desc="å¤„ç†ç« èŠ‚") as pbar:
        with concurrent.futures.ThreadPoolExecutor(max_workers=this_workers) as executor:  # å‡å°‘å¹¶å‘æ•°é¿å…è¶…é™
            futures = {}

            # æäº¤ç« èŠ‚çº§ä»»åŠ¡
            print(f"[DEBUG] selectedç« èŠ‚æ•°: {len(selected)}")
            print(f"[DEBUG] headingsç« èŠ‚æ•°: {len(headings)}")
            print(f"[DEBUG] selectedç« èŠ‚ç¤ºä¾‹: {list(selected)[:5]}")
            print(f"[DEBUG] headingsç« èŠ‚ç¤ºä¾‹: {list(headings)[:5]}")
            for heading in selected:
                if heading in headings:
                    if not headings[heading].strip():
                        print(f"[WARN] {heading} åœ¨headingsä¸­ä½†å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    sentences = [s for s in re.split(r'(?<=[.!?])\s+', headings[heading]) if s.strip()]
                    if not sentences:
                        print(f"[WARN] {heading} å†…å®¹åˆ†å¥åä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    futures[executor.submit(process_sentence, client, heading, sentences)] = heading
                else:
                    print(f"[WARN] {heading} ä¸åœ¨headingsä¸­ï¼Œè·³è¿‡")

            # å¤„ç†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                heading = futures[future]
                try:
                    optimized_text = future.result()
                    results[heading] = optimized_text

                    # è®°å½•åˆ°DataFrame
                    original_sentences = re.split(r'(?<=[.!?])\s+', headings[heading])
                    optimized_sentences = re.split(r'(?<=[.!?])\s+', optimized_text)
                    for orig, opt in zip(original_sentences, optimized_sentences):
                        df_data.append([heading, orig, opt])

                except Exception as e:
                    print(f"âŒ ç« èŠ‚å¤„ç†å¤±è´¥: {heading} - {str(e)}")
                    results[heading] = headings[heading]  # ä¿ç•™åŸå§‹å†…å®¹
                finally:
                    pbar.update(1)

    # ä¿å­˜ç»“æœ
    pd.DataFrame(df_data, columns=["ç« èŠ‚", "åŸå¥", "ä¿®æ­£"]).to_excel(
        config["paths"]["output_excel"], index=False)

    with open(config["paths"]["output_json"], "w", encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
